{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69e719ef-f645-4784-9766-47bda59d096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import configparser\n",
    "import psycopg2\n",
    "from psycopg2 import extras\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Logging ---\n",
    "def setup_logging(log_file):\n",
    "    \"\"\"Sets up logging configuration.\"\"\"\n",
    "    if not os.path.exists(os.path.dirname(log_file)):\n",
    "        os.makedirs(os.path.dirname(log_file))\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "# --- Database Connection ---\n",
    "def get_db_connection(config_file='config.ini'):\n",
    "    \"\"\"Establishes and returns a database connection.\"\"\"\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    db_config = config['DATABASE']\n",
    "\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=db_config['DB_HOST'],\n",
    "            database=db_config['DB_NAME'],\n",
    "            user=db_config['DB_USER'],\n",
    "            password=db_config['DB_PASSWORD'],\n",
    "            port=db_config['DB_PORT']\n",
    "        )\n",
    "        return conn\n",
    "    except psycopg2.Error as e:\n",
    "        logger.error(f\"Error connecting to database: {e}\")\n",
    "        raise\n",
    "\n",
    "# --- Config Loader ---\n",
    "def load_config(config_file='config.ini'):\n",
    "    \"\"\"Loads configuration from config.ini.\"\"\"\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    return config\n",
    "\n",
    "# --- Timestamp Management ---\n",
    "def get_last_run_timestamp(file_path):\n",
    "    \"\"\"Reads the timestamp of the last successful ETL run.\"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            ts_str = f.read().strip()\n",
    "            if ts_str:\n",
    "                return datetime.fromisoformat(ts_str)\n",
    "    return None\n",
    "\n",
    "def set_last_run_timestamp(file_path, timestamp):\n",
    "    \"\"\"Writes the current timestamp as the last successful ETL run.\"\"\"\n",
    "    if not os.path.exists(os.path.dirname(file_path)):\n",
    "        os.makedirs(os.path.dirname(file_path))\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(timestamp.isoformat())\n",
    "\n",
    "logger = setup_logging('logs/etl_pipeline.log') # Inisialisasi logger di sini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0079bfcc-61b4-4b7c-9977-2be3fd36036e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Downloading psycopg2-2.9.11-cp313-cp313-win_amd64.whl.metadata (5.1 kB)\n",
      "Downloading psycopg2-2.9.11-cp313-cp313-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.7 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.8/2.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.6/2.7 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.1/2.7 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 2.9 MB/s  0:00:00\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de48e3a-a4ff-406f-9098-bbe6be35a5cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_db_connection' from 'utils' (C:\\Users\\62818\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_db_connection, get_last_run_timestamp, set_last_run_timestamp, logger\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_data\u001b[39m(table_name, last_run_timestamp=\u001b[38;5;28;01mNone\u001b[39;00m, batch_size=\u001b[38;5;28;01mNone\u001b[39;00m, id_column=\u001b[38;5;28;01mNone\u001b[39;00m, timestamp_column=\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'get_db_connection' from 'utils' (C:\\Users\\62818\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils import get_db_connection, get_last_run_timestamp, set_last_run_timestamp, logger\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def extract_data(table_name, last_run_timestamp=None, batch_size=None, id_column=None, timestamp_column=None):\n",
    "    \"\"\"\n",
    "    Extracts data from a given table.\n",
    "    Performs incremental load if last_run_timestamp and timestamp_column are provided.\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = get_db_connection()\n",
    "        query = f\"SELECT * FROM {table_name}\"\n",
    "        if last_run_timestamp and timestamp_column:\n",
    "            query += f\" WHERE {timestamp_column} > '{last_run_timestamp.isoformat()}'\"\n",
    "            logger.info(f\"Performing incremental load for {table_name} since {last_run_timestamp.isoformat()}\")\n",
    "        else:\n",
    "            logger.info(f\"Performing full load for {table_name}\")\n",
    "\n",
    "        query += \" ORDER BY \"\n",
    "        if timestamp_column:\n",
    "            query += f\"{timestamp_column} ASC\"\n",
    "        elif id_column:\n",
    "            query += f\"{id_column} ASC\"\n",
    "        else:\n",
    "            query += f\"1 ASC\" # Default to first column\n",
    "\n",
    "        # If batch_size is specified and not full load, fetch incrementally\n",
    "        if batch_size and last_run_timestamp and timestamp_column:\n",
    "            # This part would be more complex for true incremental batching\n",
    "            # For simplicity, we fetch all new data if timestamp is used\n",
    "            # For very large tables, you'd iterate with LIMIT/OFFSET or id_column paging\n",
    "            df = pd.read_sql(query, conn)\n",
    "            logger.info(f\"Extracted {len(df)} new records from {table_name}.\")\n",
    "        else:\n",
    "            df = pd.read_sql(query, conn)\n",
    "            logger.info(f\"Extracted {len(df)} records from {table_name}.\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during data extraction from {table_name}: {e}\")\n",
    "        return pd.DataFrame() # Return empty DataFrame on error\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "def handle_data_quality(df, table_name):\n",
    "    \"\"\"\n",
    "    Basic data quality checks and handling.\n",
    "    This is a placeholder and should be expanded based on specific data issues.\n",
    "    \"\"\"\n",
    "    initial_rows = len(df)\n",
    "    if df.empty:\n",
    "        logger.warning(f\"No data to process for {table_name}. Skipping quality checks.\")\n",
    "        return df\n",
    "\n",
    "    # Example 1: Drop rows with critical NaNs (e.g., product_id, quantity)\n",
    "    if 'product_id' in df.columns and 'quantity' in df.columns:\n",
    "        df.dropna(subset=['product_id', 'quantity'], inplace=True)\n",
    "        if len(df) < initial_rows:\n",
    "            logger.warning(f\"Dropped {initial_rows - len(df)} rows due to critical NaNs in {table_name}.\")\n",
    "            initial_rows = len(df)\n",
    "\n",
    "    # Example 2: Convert types if necessary (Pandas read_sql usually handles this well)\n",
    "    # Ensure quantity is integer\n",
    "    if 'quantity' in df.columns:\n",
    "        df['quantity'] = pd.to_numeric(df['quantity'], errors='coerce').fillna(0).astype(int)\n",
    "    if 'unit_price' in df.columns:\n",
    "        df['unit_price'] = pd.to_numeric(df['unit_price'], errors='coerce').fillna(0.0)\n",
    "\n",
    "    # Example 3: Remove duplicates if a natural key exists\n",
    "    # e.g., for stocks, product_id + warehouse_id should be unique\n",
    "    if table_name == 'stocks' and 'product_id' in df.columns and 'warehouse_id' in df.columns:\n",
    "        df.drop_duplicates(subset=['product_id', 'warehouse_id'], inplace=True)\n",
    "        if len(df) < initial_rows:\n",
    "            logger.warning(f\"Dropped {initial_rows - len(df)} duplicate stock records.\")\n",
    "            initial_rows = len(df)\n",
    "\n",
    "    logger.info(f\"Data quality check completed for {table_name}. Remaining rows: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "def run_extraction(config):\n",
    "    \"\"\"Orchestrates data extraction for all necessary tables.\"\"\"\n",
    "    last_run_timestamp_file = config['ETL']['LAST_RUN_FILE']\n",
    "    last_run_ts = get_last_run_timestamp(last_run_timestamp_file)\n",
    "\n",
    "    extracted_data = {}\n",
    "    tables_to_extract = {\n",
    "        'warehouses': {'id_col': 'warehouse_id'},\n",
    "        'products': {'id_col': 'product_id'},\n",
    "        'categories': {'id_col': 'category_id'},\n",
    "        'suppliers': {'id_col': 'supplier_id'},\n",
    "        'stocks': {'id_col': 'stock_id', 'ts_col': 'last_updated'},\n",
    "        'stock_movements': {'id_col': 'movement_id', 'ts_col': 'movement_date'},\n",
    "        'stock_receipt_costs': {'id_col': 'receipt_id', 'ts_col': 'receipt_date'},\n",
    "        'reorder_points': {'id_col': 'reorder_point_id'},\n",
    "        # Add other tables like purchase_orders, sales_orders if needed for specific metrics\n",
    "        'purchase_orders': {'id_col': 'id', 'ts_col': 'order_date'}, # Assuming 'id' is PK and 'order_date' is timestamp\n",
    "        'purchase_order_details': {'id_col': 'id'},\n",
    "        'sales_orders': {'id_col': 'id', 'ts_col': 'order_date'},\n",
    "        'sales_order_details': {'id_col': 'id'},\n",
    "    }\n",
    "    \n",
    "    current_run_timestamp = datetime.now() # Mark this run's timestamp\n",
    "\n",
    "    for table_name, params in tables_to_extract.items():\n",
    "        df = extract_data(\n",
    "            table_name=table_name,\n",
    "            last_run_timestamp=last_run_ts if params.get('ts_col') else None,\n",
    "            id_column=params.get('id_col'),\n",
    "            timestamp_column=params.get('ts_col')\n",
    "        )\n",
    "        df = handle_data_quality(df, table_name)\n",
    "        extracted_data[table_name] = df\n",
    "\n",
    "    # Update the last run timestamp after successful extraction of all tables\n",
    "    set_last_run_timestamp(last_run_timestamp_file, current_run_timestamp)\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ecfb9cf-a5e1-4691-9e83-304c7f067da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting utils\n",
      "  Downloading utils-1.0.2.tar.gz (13 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: utils\n",
      "  Building wheel for utils (pyproject.toml): started\n",
      "  Building wheel for utils (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for utils: filename=utils-1.0.2-py2.py3-none-any.whl size=14012 sha256=ecc3b2e681e38d2283f43af46890f57278b693eedb650d905fab1916711722f5\n",
      "  Stored in directory: c:\\users\\62818\\appdata\\local\\pip\\cache\\wheels\\cd\\95\\b5\\2513b327cdc0ef5f9b087f0596bff56100e7ec84e0d0f4ed18\n",
      "Successfully built utils\n",
      "Installing collected packages: utils\n",
      "Successfully installed utils-1.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42b9128d-fca8-47bd-9015-bad384ffb1a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'logger' from 'utils' (C:\\Users\\62818\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalculate_inventory_metrics\u001b[39m(data, config):\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'logger' from 'utils' (C:\\Users\\62818\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from utils import logger\n",
    "import numpy as np\n",
    "\n",
    "def calculate_inventory_metrics(data, config):\n",
    "    \"\"\"\n",
    "    Calculates various inventory-related metrics.\n",
    "    Assumes 'stocks', 'products', 'stock_movements', 'stock_receipt_costs' are in data.\n",
    "    \"\"\"\n",
    "    logger.info(\"Calculating Inventory Metrics...\")\n",
    "    df_stocks = data.get('stocks', pd.DataFrame())\n",
    "    df_products = data.get('products', pd.DataFrame())\n",
    "    df_movements = data.get('stock_movements', pd.DataFrame())\n",
    "    df_receipts = data.get('stock_receipt_costs', pd.DataFrame())\n",
    "\n",
    "    if df_stocks.empty or df_products.empty:\n",
    "        logger.warning(\"Missing 'stocks' or 'products' data for inventory metrics.\")\n",
    "        return {}\n",
    "\n",
    "    # Merge product info for unit price\n",
    "    df_stocks = pd.merge(df_stocks, df_products[['product_id', 'unit_price']], on='product_id', how='left')\n",
    "    \n",
    "    metrics = {}\n",
    "\n",
    "    # Stock turnover ratio per product/category\n",
    "    # Sum of sales (OUT movements) in a period / Average inventory value in that period\n",
    "    # This needs sales data and average stock value. For simplicity, let's use current stock value and OUT movements.\n",
    "    if not df_movements.empty:\n",
    "        sales_movements = df_movements[df_movements['movement_type'].str.contains('OUT', na=False)]\n",
    "        sales_qty_per_product = sales_movements.groupby('product_id')['quantity'].sum().reset_index()\n",
    "        sales_qty_per_product.rename(columns={'quantity': 'total_out_qty'}, inplace=True)\n",
    "\n",
    "        df_turnover = pd.merge(df_stocks, sales_qty_per_product, on='product_id', how='left').fillna(0)\n",
    "        df_turnover['avg_inventory_qty'] = df_turnover['quantity'] # Simplification: use current as avg\n",
    "        df_turnover['turnover_ratio'] = df_turnover['total_out_qty'] / df_turnover['avg_inventory_qty']\n",
    "        \n",
    "        metrics['stock_turnover_ratio_product'] = df_turnover[['product_id', 'turnover_ratio']]\n",
    "        logger.info(\"Calculated stock turnover ratio per product.\")\n",
    "\n",
    "    # Days of inventory on hand\n",
    "    # (Current Inventory / Cost of Goods Sold per day)\n",
    "    # COGS per day needs more info. Using average daily OUT quantity as proxy for now.\n",
    "    if not df_movements.empty and not df_stocks.empty:\n",
    "        period_days = (datetime.now() - df_movements['movement_date'].min()).days\n",
    "        if period_days == 0: period_days = 1 # Avoid division by zero\n",
    "        \n",
    "        sales_qty_per_product_daily_avg = df_movements[df_movements['movement_type'].str.contains('OUT', na=False)] \\\n",
    "                                          .groupby('product_id')['quantity'].sum().reset_index()\n",
    "        sales_qty_per_product_daily_avg['avg_daily_out_qty'] = sales_qty_per_product_daily_avg['quantity'] / period_days\n",
    "        \n",
    "        df_days_on_hand = pd.merge(df_stocks, sales_qty_per_product_daily_avg, on='product_id', how='left').fillna(0)\n",
    "        df_days_on_hand['days_of_inventory_on_hand'] = df_days_on_hand['quantity'] / df_days_on_hand['avg_daily_out_qty']\n",
    "        df_days_on_hand.replace([np.inf, -np.inf], np.nan, inplace=True) # Handle division by zero\n",
    "        \n",
    "        metrics['days_of_inventory_on_hand_product'] = df_days_on_hand[['product_id', 'days_of_inventory_on_hand']]\n",
    "        logger.info(\"Calculated days of inventory on hand per product.\")\n",
    "\n",
    "    # Dead stock identification (no movement >180 days)\n",
    "    dead_stock_days = int(config['METRICS']['DEAD_STOCK_DAYS'])\n",
    "    threshold_date = datetime.now() - timedelta(days=dead_stock_days)\n",
    "\n",
    "    if not df_movements.empty and not df_stocks.empty:\n",
    "        last_movement_date = df_movements.groupby('product_id')['movement_date'].max().reset_index()\n",
    "        last_movement_date.rename(columns={'movement_date': 'last_movement_date'}, inplace=True)\n",
    "\n",
    "        df_dead_stock = pd.merge(df_stocks, last_movement_date, on='product_id', how='left')\n",
    "        \n",
    "        # Consider products with no movement history (NaN last_movement_date) as potentially dead if they have stock\n",
    "        df_dead_stock['is_dead_stock'] = (\n",
    "            (df_dead_stock['last_movement_date'].isna() & (df_dead_stock['quantity'] > 0)) |\n",
    "            (df_dead_stock['last_movement_date'] < threshold_date)\n",
    "        )\n",
    "        metrics['dead_stock_identification'] = df_dead_stock[df_dead_stock['is_dead_stock']][['product_id', 'warehouse_id', 'quantity', 'last_movement_date']]\n",
    "        logger.info(f\"Identified dead stock (no movement > {dead_stock_days} days).\")\n",
    "\n",
    "    # Stock accuracy (physical vs system) - requires external \"physical count\" data, so skipped for now.\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def calculate_movement_analytics(data):\n",
    "    \"\"\"\n",
    "    Calculates various movement-related analytics.\n",
    "    Assumes 'stock_movements' is in data.\n",
    "    \"\"\"\n",
    "    logger.info(\"Calculating Movement Analytics...\")\n",
    "    df_movements = data.get('stock_movements', pd.DataFrame())\n",
    "\n",
    "    if df_movements.empty:\n",
    "        logger.warning(\"Missing 'stock_movements' data for movement analytics.\")\n",
    "        return {}\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    df_movements['movement_date'] = pd.to_datetime(df_movements['movement_date'])\n",
    "    df_movements['date'] = df_movements['movement_date'].dt.date\n",
    "    df_movements['week'] = df_movements['movement_date'].dt.isocalendar().week\n",
    "    df_movements['month'] = df_movements['movement_date'].dt.month\n",
    "    df_movements['hour'] = df_movements['movement_date'].dt.hour\n",
    "    df_movements['movement_value'] = df_movements['quantity'] # For sum/avg calculations\n",
    "\n",
    "    # Average daily movement per product\n",
    "    daily_movement_product = df_movements.groupby(['product_id', 'date'])['movement_value'].sum().reset_index()\n",
    "    avg_daily_movement_product = daily_movement_product.groupby('product_id')['movement_value'].mean().reset_index()\n",
    "    avg_daily_movement_product.rename(columns={'movement_value': 'avg_daily_movement'}, inplace=True)\n",
    "    metrics['avg_daily_movement_product'] = avg_daily_movement_product\n",
    "    logger.info(\"Calculated average daily movement per product.\")\n",
    "\n",
    "    # Peak periods identification (hourly, daily, weekly, monthly)\n",
    "    metrics['peak_hourly_movement'] = df_movements.groupby('hour')['movement_value'].sum().reset_index().sort_values(by='movement_value', ascending=False)\n",
    "    metrics['peak_daily_movement'] = df_movements.groupby('date')['movement_value'].sum().reset_index().sort_values(by='movement_value', ascending=False)\n",
    "    metrics['peak_weekly_movement'] = df_movements.groupby('week')['movement_value'].sum().reset_index().sort_values(by='movement_value', ascending=False)\n",
    "    metrics['peak_monthly_movement'] = df_movements.groupby('month')['movement_value'].sum().reset_index().sort_values(by='movement_value', ascending=False)\n",
    "    logger.info(\"Identified peak movement periods.\")\n",
    "\n",
    "    # Movement trends (daily, weekly, monthly) - Aggregations\n",
    "    metrics['daily_movement_trend'] = df_movements.groupby('date')['movement_value'].sum().reset_index()\n",
    "    metrics['weekly_movement_trend'] = df_movements.groupby('week')['movement_value'].sum().reset_index()\n",
    "    metrics['monthly_movement_trend'] = df_movements.groupby('month')['movement_value'].sum().reset_index()\n",
    "    logger.info(\"Calculated movement trends.\")\n",
    "\n",
    "    # Seasonal patterns detection (basic: avg movement per month)\n",
    "    metrics['seasonal_monthly_avg_movement'] = df_movements.groupby('month')['movement_value'].mean().reset_index()\n",
    "    logger.info(\"Detected basic seasonal patterns.\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def calculate_warehouse_performance(data):\n",
    "    \"\"\"\n",
    "    Calculates warehouse performance metrics.\n",
    "    Assumes 'warehouses', 'stocks', 'stock_movements' are in data.\n",
    "    \"\"\"\n",
    "    logger.info(\"Calculating Warehouse Performance...\")\n",
    "    df_warehouses = data.get('warehouses', pd.DataFrame())\n",
    "    df_stocks = data.get('stocks', pd.DataFrame())\n",
    "    df_movements = data.get('stock_movements', pd.DataFrame())\n",
    "\n",
    "    if df_warehouses.empty:\n",
    "        logger.warning(\"Missing 'warehouses' data for warehouse performance.\")\n",
    "        return {}\n",
    "    \n",
    "    metrics = {}\n",
    "\n",
    "    # Utilization rate per warehouse (requires warehouse capacity data, which we don't have)\n",
    "    # Placeholder: total quantity stored\n",
    "    warehouse_stock_summary = df_stocks.groupby('warehouse_id')['quantity'].sum().reset_index()\n",
    "    metrics['warehouse_total_stock_qty'] = warehouse_stock_summary\n",
    "    logger.info(\"Calculated total stock quantity per warehouse (utilization proxy).\")\n",
    "\n",
    "    # In/Out efficiency\n",
    "    if not df_movements.empty:\n",
    "        in_out_movements = df_movements.groupby(['warehouse_id', 'movement_type'])['quantity'].sum().reset_index()\n",
    "        in_movements = in_out_movements[in_out_movements['movement_type'].str.contains('IN', na=False)]\n",
    "        out_movements = in_out_movements[in_out_movements['movement_type'].str.contains('OUT', na=False)]\n",
    "\n",
    "        in_movements.rename(columns={'quantity': 'total_in_qty'}, inplace=True)\n",
    "        out_movements.rename(columns={'quantity': 'total_out_qty'}, inplace=True)\n",
    "\n",
    "        df_efficiency = pd.merge(\n",
    "            in_movements[['warehouse_id', 'total_in_qty']],\n",
    "            out_movements[['warehouse_id', 'total_out_qty']],\n",
    "            on='warehouse_id', how='outer'\n",
    "        ).fillna(0)\n",
    "        df_efficiency['in_out_ratio'] = df_efficiency['total_in_qty'] / df_efficiency['total_out_qty']\n",
    "        df_efficiency.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "        metrics['warehouse_in_out_efficiency'] = df_efficiency\n",
    "        logger.info(\"Calculated warehouse In/Out efficiency.\")\n",
    "\n",
    "    # Transfer patterns between warehouses\n",
    "    if not df_movements.empty:\n",
    "        df_movements_copy = df_movements.copy()\n",
    "        df_movements_copy['from_warehouse_id'] = np.where(df_movements_copy['movement_type'] == 'OUT', df_movements_copy['warehouse_id'], np.nan)\n",
    "        df_movements_copy['to_warehouse_id'] = np.where(df_movements_copy['movement_type'] == 'IN', df_movements_copy['warehouse_id'], np.nan)\n",
    "\n",
    "        # To properly track transfers, a specific 'TRANSFER' type in movement_type with FROM/TO fields would be better.\n",
    "        # For this setup, we'd need to link IN/OUT movements that belong to the same transfer.\n",
    "        # This is a simplification:\n",
    "        transfer_outs = df_movements[(df_movements['movement_type'] == 'OUT') & (df_movements['notes'].str.contains('Transfer OUT', na=False))]\n",
    "        transfer_ins = df_movements[(df_movements['movement_type'] == 'IN') & (df_movements['notes'].str.contains('Transfer IN', na=False))]\n",
    "\n",
    "        # A robust solution would pair these based on product_id, quantity, and close timestamps.\n",
    "        # For simplicity, we just aggregate transfer quantities.\n",
    "        metrics['total_transfer_out_qty'] = transfer_outs.groupby('warehouse_id')['quantity'].sum().reset_index()\n",
    "        metrics['total_transfer_in_qty'] = transfer_ins.groupby('warehouse_id')['quantity'].sum().reset_index()\n",
    "        logger.info(\"Calculated basic transfer patterns.\")\n",
    "\n",
    "\n",
    "    # Geographic distribution optimization - requires geographic data (lat/long) and demand points. Skipped for now.\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def calculate_financial_metrics(data):\n",
    "    \"\"\"\n",
    "    Calculates financial metrics.\n",
    "    Assumes 'stocks', 'stock_receipt_costs', 'products' are in data.\n",
    "    \"\"\"\n",
    "    logger.info(\"Calculating Financial Metrics...\")\n",
    "    df_stocks = data.get('stocks', pd.DataFrame())\n",
    "    df_receipts = data.get('stock_receipt_costs', pd.DataFrame())\n",
    "    df_products = data.get('products', pd.DataFrame())\n",
    "    \n",
    "    metrics = {}\n",
    "\n",
    "    if df_stocks.empty or df_products.empty or df_receipts.empty:\n",
    "        logger.warning(\"Missing 'stocks', 'products', or 'stock_receipt_costs' data for financial metrics.\")\n",
    "        return {}\n",
    "\n",
    "    # Inventory value over time (snapshot of current value using AVG method for simplicity)\n",
    "    # This function uses the `calculate_stock_value` PostgreSQL function, so we need a DB connection.\n",
    "    # Alternatively, replicate the logic here. Let's replicate AVG logic for Python.\n",
    "    \n",
    "    # Merge current stock with product details\n",
    "    df_current_stock_details = pd.merge(df_stocks, df_products[['product_id', 'name']], on='product_id', how='left')\n",
    "    \n",
    "    # Calculate Average Cost for each product in each warehouse based on remaining receipts\n",
    "    avg_costs_data = []\n",
    "    for (product_id, warehouse_id), group in df_receipts.groupby(['product_id', 'warehouse_id']):\n",
    "        remaining_qty = group['remaining_quantity'].sum()\n",
    "        total_remaining_cost = (group['remaining_quantity'] * group['unit_cost']).sum()\n",
    "        \n",
    "        if remaining_qty > 0:\n",
    "            avg_unit_cost = total_remaining_cost / remaining_qty\n",
    "            avg_costs_data.append({'product_id': product_id, 'warehouse_id': warehouse_id, 'avg_unit_cost': avg_unit_cost})\n",
    "    \n",
    "    df_avg_costs = pd.DataFrame(avg_costs_data)\n",
    "\n",
    "    df_inventory_value = pd.merge(\n",
    "        df_current_stock_details, \n",
    "        df_avg_costs, \n",
    "        on=['product_id', 'warehouse_id'], \n",
    "        how='left'\n",
    "    ).fillna(0) # Fill NaN avg_unit_cost with 0 if no receipts\n",
    "\n",
    "    df_inventory_value['item_value'] = df_inventory_value['quantity'] * df_inventory_value['avg_unit_cost']\n",
    "    metrics['inventory_value_overview'] = df_inventory_value[['product_id', 'warehouse_id', 'quantity', 'avg_unit_cost', 'item_value']]\n",
    "    metrics['total_inventory_value'] = df_inventory_value['item_value'].sum()\n",
    "    logger.info(\"Calculated inventory value (using AVG method).\")\n",
    "\n",
    "\n",
    "    # Holding cost calculation (requires holding cost percentage, e.g., 15-30% of inventory value annually)\n",
    "    # Assume 20% annual holding cost for simplicity\n",
    "    annual_holding_cost_rate = 0.20\n",
    "    if 'total_inventory_value' in metrics:\n",
    "        metrics['estimated_annual_holding_cost'] = metrics['total_inventory_value'] * annual_holding_cost_rate\n",
    "    logger.info(\"Calculated estimated holding cost.\")\n",
    "\n",
    "\n",
    "    # Stock-out cost estimation (requires average profit margin per product and estimated lost sales due to stock-out)\n",
    "    # This is highly theoretical and requires sales data + margin. Skipped for now.\n",
    "    \n",
    "    # ABC analysis (Pareto classification)\n",
    "    # Classify products by their contribution to total inventory value (A: 80%, B: 15%, C: 5%)\n",
    "    if not metrics['inventory_value_overview'].empty:\n",
    "        df_abc = metrics['inventory_value_overview'].groupby('product_id')['item_value'].sum().reset_index()\n",
    "        df_abc = df_abc.sort_values(by='item_value', ascending=False).reset_index(drop=True)\n",
    "        df_abc['cumulative_value'] = df_abc['item_value'].cumsum()\n",
    "        df_abc['cumulative_percent'] = (df_abc['cumulative_value'] / df_abc['item_value'].sum()) * 100\n",
    "\n",
    "        def assign_abc_class(row):\n",
    "            if row['cumulative_percent'] <= 80:\n",
    "                return 'A'\n",
    "            elif row['cumulative_percent'] <= 95: # 80 + 15 = 95\n",
    "                return 'B'\n",
    "            else:\n",
    "                return 'C'\n",
    "        df_abc['abc_class'] = df_abc.apply(assign_abc_class, axis=1)\n",
    "        metrics['abc_analysis'] = pd.merge(df_abc, df_products[['product_id', 'name']], on='product_id', how='left')\n",
    "        logger.info(\"Performed ABC analysis.\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def run_transformations(extracted_data, config):\n",
    "    \"\"\"Orchestrates all transformations.\"\"\"\n",
    "    logger.info(\"Starting data transformations...\")\n",
    "    transformed_data = {}\n",
    "\n",
    "    transformed_data.update(calculate_inventory_metrics(extracted_data, config))\n",
    "    transformed_data.update(calculate_movement_analytics(extracted_data))\n",
    "    transformed_data.update(calculate_warehouse_performance(extracted_data))\n",
    "    transformed_data.update(calculate_financial_metrics(extracted_data))\n",
    "\n",
    "    logger.info(\"Data transformations completed.\")\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72a96ce3-0d27-4f63-9d4c-927dc8ae0cd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'logger' from 'utils' (C:\\Users\\62818\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logger, load_config\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'logger' from 'utils' (C:\\Users\\62818\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils import logger, load_config\n",
    "import os\n",
    "from datetime import datetime\n",
    "import jinja2 # Untuk template HTML\n",
    "# import pdfkit # Untuk PDF, perlu wkhtmltopdf terinstal\n",
    "\n",
    "def create_summary_tables(transformed_data, db_conn):\n",
    "    \"\"\"\n",
    "    Loads transformed data into summary tables in the database.\n",
    "    This is a conceptual example; actual DDL would be needed.\n",
    "    \"\"\"\n",
    "    logger.info(\"Creating summary tables/materialized views in database...\")\n",
    "    cursor = db_conn.cursor()\n",
    "\n",
    "    try:\n",
    "        # Example: inventory_value_summary\n",
    "        if 'inventory_value_overview' in transformed_data:\n",
    "            df_summary = transformed_data['inventory_value_overview']\n",
    "            table_name = 'analytics_inventory_value_summary'\n",
    "            logger.info(f\"Loading '{table_name}' with {len(df_summary)} records.\")\n",
    "            # Drop/create table DDL would go here\n",
    "            # Example: CREATE TABLE analytics_inventory_value_summary (product_id INT, warehouse_id INT, ...);\n",
    "            \n",
    "            # Using pandas to_sql for simplicity for loading\n",
    "            df_summary.to_sql(table_name, db_conn, if_exists='replace', index=False)\n",
    "            logger.info(f\"Successfully loaded {table_name}.\")\n",
    "\n",
    "        # Example: abc_analysis_summary\n",
    "        if 'abc_analysis' in transformed_data:\n",
    "            df_abc = transformed_data['abc_analysis']\n",
    "            table_name = 'analytics_abc_analysis'\n",
    "            logger.info(f\"Loading '{table_name}' with {len(df_abc)} records.\")\n",
    "            df_abc.to_sql(table_name, db_conn, if_exists='replace', index=False)\n",
    "            logger.info(f\"Successfully loaded {table_name}.\")\n",
    "\n",
    "        # Add other summary tables as needed\n",
    "        \n",
    "        db_conn.commit()\n",
    "        logger.info(\"Summary tables/materialized views creation complete.\")\n",
    "    except Exception as e:\n",
    "        db_conn.rollback()\n",
    "        logger.error(f\"Error creating summary tables: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "def export_to_analytics_ready_format(transformed_data, export_dir):\n",
    "    \"\"\"\n",
    "    Exports transformed data to Parquet/CSV files.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Exporting analytics-ready data to {export_dir}...\")\n",
    "    if not os.path.exists(export_dir):\n",
    "        os.makedirs(export_dir)\n",
    "\n",
    "    for metric_name, df_or_value in transformed_data.items():\n",
    "        if isinstance(df_or_value, pd.DataFrame) and not df_or_value.empty:\n",
    "            file_path_csv = os.path.join(export_dir, f\"{metric_name}.csv\")\n",
    "            # file_path_parquet = os.path.join(export_dir, f\"{metric_name}.parquet\")\n",
    "            \n",
    "            df_or_value.to_csv(file_path_csv, index=False)\n",
    "            # df_or_value.to_parquet(file_path_parquet, index=False) # Uncomment if you want parquet\n",
    "            logger.info(f\"Exported {metric_name} to {file_path_csv}.\")\n",
    "        elif not isinstance(df_or_value, pd.DataFrame):\n",
    "            # Handle single values\n",
    "            logger.info(f\"Skipping export for scalar metric '{metric_name}': {df_or_value}\")\n",
    "    logger.info(\"Data export complete.\")\n",
    "\n",
    "def generate_automated_reports(transformed_data, output_dir):\n",
    "    \"\"\"\n",
    "    Generates automated reports in HTML (or PDF if wkhtmltopdf is installed).\n",
    "    \"\"\"\n",
    "    logger.info(f\"Generating automated reports to {output_dir}...\")\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    template_loader = jinja2.FileSystemLoader(searchpath=\"./templates\") # Assume templates folder exists\n",
    "    template_env = jinja2.Environment(loader=template_loader)\n",
    "    \n",
    "    # Load HTML template\n",
    "    try:\n",
    "        template = template_env.get_template(\"report_template.html\")\n",
    "    except jinja2.exceptions.TemplateNotFound:\n",
    "        logger.error(\"report_template.html not found in ./templates directory. Skipping HTML report generation.\")\n",
    "        return\n",
    "\n",
    "    # Prepare data for template\n",
    "    report_data = {\n",
    "        'report_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        'total_inventory_value': transformed_data.get('total_inventory_value', 0),\n",
    "        'estimated_annual_holding_cost': transformed_data.get('estimated_annual_holding_cost', 0),\n",
    "        'abc_analysis': transformed_data.get('abc_analysis', pd.DataFrame()).to_html(index=False),\n",
    "        'stock_turnover_ratio_product': transformed_data.get('stock_turnover_ratio_product', pd.DataFrame()).head(10).to_html(index=False),\n",
    "        'dead_stock_identification': transformed_data.get('dead_stock_identification', pd.DataFrame()).to_html(index=False),\n",
    "        'peak_daily_movement': transformed_data.get('peak_daily_movement', pd.DataFrame()).head(10).to_html(index=False),\n",
    "        # Add more dataframes as needed\n",
    "    }\n",
    "\n",
    "    # Render template\n",
    "    html_report = template.render(report_data)\n",
    "    \n",
    "    report_filename = os.path.join(output_dir, f\"inventory_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html\")\n",
    "    with open(report_filename, \"w\") as f:\n",
    "        f.write(html_report)\n",
    "    logger.info(f\"HTML report generated at {report_filename}\")\n",
    "\n",
    "    # Optional: Generate PDF (requires wkhtmltopdf and pdfkit library)\n",
    "    # try:\n",
    "    #     pdf_filename = os.path.join(output_dir, f\"inventory_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf\")\n",
    "    #     pdfkit.from_file(report_filename, pdf_filename)\n",
    "    #     logger.info(f\"PDF report generated at {pdf_filename}\")\n",
    "    # except Exception as e:\n",
    "    #     logger.warning(f\"Could not generate PDF report (wkhtmltopdf not found or other error): {e}\")\n",
    "\n",
    "    logger.info(\"Report generation complete.\")\n",
    "\n",
    "\n",
    "def run_loading(transformed_data, config):\n",
    "    \"\"\"Orchestrates all loading processes.\"\"\"\n",
    "    db_conn = None\n",
    "    try:\n",
    "        db_conn = get_db_connection(config_file='config.ini')\n",
    "        \n",
    "        # Load summary tables (conceptual)\n",
    "        create_summary_tables(transformed_data, db_conn)\n",
    "\n",
    "        # Export to analytics-ready format\n",
    "        export_to_analytics_ready_format(transformed_data, config['ETL']['EXPORT_CSV_DIR'])\n",
    "\n",
    "        # Generate automated reports\n",
    "        generate_automated_reports(transformed_data, config['ETL']['OUTPUT_DIR'])\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during data loading: {e}\")\n",
    "    finally:\n",
    "        if db_conn:\n",
    "            db_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61c2ee31-f4af-469f-bf1a-b4d9f03564f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m setup_logging, load_config, logger, get_db_connection\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextract\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_extraction\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransform\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_transformations\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "from src.utils import setup_logging, load_config, logger, get_db_connection\n",
    "from src.extract import run_extraction\n",
    "from src.transform import run_transformations\n",
    "from src.load import run_loading\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the ETL pipeline.\"\"\"\n",
    "    config = load_config()\n",
    "    setup_logging(config['ETL']['LOG_FILE']) # Setup logging globally\n",
    "    logger.info(\"ETL Pipeline Started.\")\n",
    "\n",
    "    try:\n",
    "        # Create necessary directories\n",
    "        os.makedirs(config['ETL']['EXPORT_CSV_DIR'], exist_ok=True)\n",
    "        os.makedirs(config['ETL']['OUTPUT_DIR'], exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(config['ETL']['LAST_RUN_FILE']), exist_ok=True)\n",
    "\n",
    "\n",
    "        # 1. Extraction\n",
    "        extracted_data = run_extraction(config)\n",
    "        \n",
    "        # Check if any critical data is missing after extraction\n",
    "        if not extracted_data.get('stocks', pd.DataFrame()).empty and \\\n",
    "           not extracted_data.get('stock_movements', pd.DataFrame()).empty:\n",
    "            \n",
    "            # 2. Transformation\n",
    "            transformed_data = run_transformations(extracted_data, config)\n",
    "\n",
    "            # 3. Loading\n",
    "            run_loading(transformed_data, config)\n",
    "        else:\n",
    "            logger.warning(\"Skipping transformation and loading due to insufficient extracted data.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"ETL Pipeline Failed: {e}\", exc_info=True)\n",
    "        sys.exit(1)\n",
    "    \n",
    "    logger.info(\"ETL Pipeline Completed Successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure 'templates' directory exists for reports\n",
    "    if not os.path.exists('templates'):\n",
    "        os.makedirs('templates')\n",
    "        # Create a dummy template file\n",
    "        with open('templates/report_template.html', 'w') as f:\n",
    "            f.write(\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Inventory Analytics Report</title>\n",
    "    <style>\n",
    "        body { font-family: Arial, sans-serif; margin: 20px; }\n",
    "        h1, h2 { color: #333; }\n",
    "        table { width: 100%; border-collapse: collapse; margin-bottom: 20px; }\n",
    "        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }\n",
    "        th { background-color: #f2f2f2; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Inventory Analytics Report</h1>\n",
    "    <p>Generated On: {{ report_date }}</p>\n",
    "\n",
    "    <h2>Overall Financial Metrics</h2>\n",
    "    <p>Total Inventory Value: <strong>{{ \"Rp{:,.2f}\".format(total_inventory_value) }}</strong></p>\n",
    "    <p>Estimated Annual Holding Cost: <strong>{{ \"Rp{:,.2f}\".format(estimated_annual_holding_cost) }}</strong></p>\n",
    "\n",
    "    <h2>ABC Analysis (Top Products by Value)</h2>\n",
    "    {{ abc_analysis | safe }}\n",
    "\n",
    "    <h2>Stock Turnover Ratio (Top 10 Products)</h2>\n",
    "    {{ stock_turnover_ratio_product | safe }}\n",
    "\n",
    "    <h2>Dead Stock Identification</h2>\n",
    "    {{ dead_stock_identification | safe }}\n",
    "\n",
    "    <h2>Peak Daily Movement (Top 10 Days)</h2>\n",
    "    {{ peak_daily_movement | safe }}\n",
    "\n",
    "    </body>\n",
    "</html>\n",
    "            \"\"\")\n",
    "        logger.info(\"Created a default 'report_template.html' in 'templates' directory.\")\n",
    "\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2e7f012-175b-4660-9f77-5b63ef674fa6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransform\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m calculate_inventory_metrics, calculate_movement_analytics, calculate_financial_metrics\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_config\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mTestTransformations\u001b[39;00m(unittest.TestCase):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from src.transform import calculate_inventory_metrics, calculate_movement_analytics, calculate_financial_metrics\n",
    "from src.utils import load_config\n",
    "\n",
    "class TestTransformations(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        # Setup dummy dataframes for testing\n",
    "        self.config = load_config(config_file='config.ini') # Ensure config is loaded for metrics\n",
    "\n",
    "        self.df_products = pd.DataFrame({\n",
    "            'product_id': [1, 2, 3, 4],\n",
    "            'name': ['Product A', 'Product B', 'Product C', 'Product D'],\n",
    "            'unit_price': [10.0, 20.0, 5.0, 15.0]\n",
    "        })\n",
    "        self.df_warehouses = pd.DataFrame({\n",
    "            'warehouse_id': [1, 2],\n",
    "            'name': ['WH A', 'WH B']\n",
    "        })\n",
    "        self.df_stocks = pd.DataFrame({\n",
    "            'stock_id': [101, 102, 103, 104],\n",
    "            'product_id': [1, 2, 3, 4],\n",
    "            'warehouse_id': [1, 1, 2, 2],\n",
    "            'quantity': [100, 50, 200, 0],\n",
    "            'last_updated': [datetime.now(), datetime.now(), datetime.now(), datetime.now()]\n",
    "        })\n",
    "        self.df_movements = pd.DataFrame({\n",
    "            'movement_id': [1, 2, 3, 4, 5, 6, 7],\n",
    "            'product_id': [1, 1, 2, 2, 3, 3, 4],\n",
    "            'warehouse_id': [1, 1, 1, 1, 2, 2, 2],\n",
    "            'movement_type': ['IN', 'OUT', 'IN', 'OUT', 'IN', 'OUT', 'IN'],\n",
    "            'quantity': [10, 5, 20, 10, 30, 15, 5],\n",
    "            'movement_date': [\n",
    "                datetime.now() - timedelta(days=200),\n",
    "                datetime.now() - timedelta(days=100),\n",
    "                datetime.now() - timedelta(days=250),\n",
    "                datetime.now() - timedelta(days=50),\n",
    "                datetime.now() - timedelta(days=300),\n",
    "                datetime.now() - timedelta(days=10),\n",
    "                datetime.now() - timedelta(days=5) # Recent movement for Product D\n",
    "            ],\n",
    "            'notes': ['Bought', 'Sold', 'Bought', 'Sold', 'Bought', 'Sold', 'Bought']\n",
    "        })\n",
    "        self.df_receipts = pd.DataFrame({\n",
    "            'receipt_id': [1, 2, 3, 4],\n",
    "            'product_id': [1, 1, 2, 3],\n",
    "            'warehouse_id': [1, 1, 1, 2],\n",
    "            'quantity_received': [50, 60, 70, 80],\n",
    "            'unit_cost': [9.0, 9.5, 19.0, 4.5],\n",
    "            'receipt_date': [\n",
    "                datetime.now() - timedelta(days=200),\n",
    "                datetime.now() - timedelta(days=150),\n",
    "                datetime.now() - timedelta(days=250),\n",
    "                datetime.now() - timedelta(days=300)\n",
    "            ],\n",
    "            'remaining_quantity': [50, 60, 70, 80] # Initial state, needs to be updated by OUT movements\n",
    "        })\n",
    "        \n",
    "        self.extracted_data = {\n",
    "            'products': self.df_products,\n",
    "            'warehouses': self.df_warehouses,\n",
    "            'stocks': self.df_stocks,\n",
    "            'stock_movements': self.df_movements,\n",
    "            'stock_receipt_costs': self.df_receipts\n",
    "        }\n",
    "\n",
    "    def test_calculate_inventory_metrics(self):\n",
    "        metrics = calculate_inventory_metrics(self.extracted_data, self.config)\n",
    "        self.assertIn('stock_turnover_ratio_product', metrics)\n",
    "        self.assertIn('days_of_inventory_on_hand_product', metrics)\n",
    "        self.assertIn('dead_stock_identification', metrics)\n",
    "        \n",
    "        # Test dead stock logic\n",
    "        dead_stock = metrics['dead_stock_identification']\n",
    "        # Product A: last movement 100 days ago (not dead)\n",
    "        # Product B: last movement 50 days ago (not dead)\n",
    "        # Product C: last movement 10 days ago (not dead)\n",
    "        # Product D: stock 0, but if it had stock and last movement > 180 days ago, it would be dead.\n",
    "        # With current data, product D has quantity 0, so it won't appear.\n",
    "        # Let's adjust Product A's last movement to be very old\n",
    "        self.df_movements.loc[self.df_movements['product_id'] == 1, 'movement_date'] = datetime.now() - timedelta(days=300)\n",
    "        self.extracted_data['stock_movements'] = self.df_movements # Update data\n",
    "        metrics_recalculated = calculate_inventory_metrics(self.extracted_data, self.config)\n",
    "        dead_stock_recalculated = metrics_recalculated['dead_stock_identification']\n",
    "        self.assertTrue(1 in dead_stock_recalculated['product_id'].values) # Product 1 should now be dead stock\n",
    "        self.assertFalse(2 in dead_stock_recalculated['product_id'].values)\n",
    "\n",
    "\n",
    "    def test_calculate_movement_analytics(self):\n",
    "        metrics = calculate_movement_analytics(self.extracted_data)\n",
    "        self.assertIn('avg_daily_movement_product', metrics)\n",
    "        self.assertIn('peak_hourly_movement', metrics)\n",
    "        self.assertIn('daily_movement_trend', metrics)\n",
    "\n",
    "    def test_calculate_financial_metrics(self):\n",
    "        metrics = calculate_financial_metrics(self.extracted_data)\n",
    "        self.assertIn('inventory_value_overview', metrics)\n",
    "        self.assertIn('total_inventory_value', metrics)\n",
    "        self.assertIn('estimated_annual_holding_cost', metrics)\n",
    "        self.assertIn('abc_analysis', metrics)\n",
    "\n",
    "        # Basic check for ABC analysis (Product B (20*50=1000) should be A, Product A (10*100=1000) A, Product C (5*200=1000) A, Product D (0*15=0) C)\n",
    "        # This needs more realistic values to get proper ABC distribution.\n",
    "        # Assuming our example products all have relatively high values, they might all be 'A' or 'B' depending on thresholds.\n",
    "        # Let's check if the ABC analysis dataframe is created.\n",
    "        self.assertIsInstance(metrics['abc_analysis'], pd.DataFrame)\n",
    "        self.assertIn('abc_class', metrics['abc_analysis'].columns)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b965e4eb-a7cd-40eb-8387-ffc83677e5f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
